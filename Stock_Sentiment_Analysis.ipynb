{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsF6TS0gFT8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c00637-705d-4a71-a6b0-0ed4fa948773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install nltk\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dy-5pnxRvXB",
        "outputId": "97088b1e-5f8a-4779-dbbf-da8a6b35654d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Step 1: Fetch Financial News Headlines from yfinance\n",
        "def fetch_news(ticker):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    news = stock.news\n",
        "    headlines = []\n",
        "\n",
        "    for article in news:\n",
        "        date_time = article['publishedAt']\n",
        "        title = article['title']\n",
        "        source = article['source']\n",
        "        headlines.append((date_time, source, title))\n",
        "\n",
        "    return headlines\n",
        "\n",
        "# Step 2: Prepare Data for Analysis\n",
        "def prepare_data(headlines):\n",
        "    data = pd.DataFrame(headlines, columns=['Date_Time', 'Source', 'Headline'])\n",
        "    data['Date_Time'] = pd.to_datetime(data['Date_Time'])\n",
        "    return data\n",
        "\n",
        "# Step 3: Sentiment Analysis\n",
        "def analyze_sentiment(data):\n",
        "    nltk.download('vader_lexicon')\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Augment VADER with domain-specific terms (example)\n",
        "    custom_sentiments = {\n",
        "        'bullish': 2.0,\n",
        "        'bearish': -2.0,\n",
        "        'uptrend': 1.5,\n",
        "        'downtrend': -1.5\n",
        "    }\n",
        "\n",
        "    # Add custom sentiments to VADER's lexicon\n",
        "    for word, sentiment in custom_sentiments.items():\n",
        "        sia.lexicon[word] = sentiment\n",
        "\n",
        "    data['Compound'] = data['Headline'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "    data['Sentiment'] = data['Compound'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))\n",
        "    return data\n",
        "\n",
        "# Step 4: Visualize Sentiment Trends\n",
        "def visualize_sentiment(data):\n",
        "    # Group by date and sentiment\n",
        "    sentiment_counts = data.groupby([data['Date_Time'].dt.date, 'Sentiment']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Plotting\n",
        "    sentiment_counts.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
        "    plt.title('Sentiment Distribution of Financial News Headlines')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Number of Headlines')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Sentiment')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Main function to run the project\n",
        "def main():\n",
        "    ticker = 'AAPL'  # Replace with the desired stock ticker\n",
        "    headlines = fetch_news(ticker)\n",
        "    data = prepare_data(headlines)\n",
        "    data_with_sentiment = analyze_sentiment(data)\n",
        "    visualize_sentiment(data_with_sentiment)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "QDhJnjq-_tVS",
        "outputId": "0016c53c-9ea5-4267-814e-9c2dff767eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'publishedAt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5501eee14c2c>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-5501eee14c2c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AAPL'\u001b[0m  \u001b[0;31m# Replace with the desired stock ticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mheadlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_news\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mdata_with_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5501eee14c2c>\u001b[0m in \u001b[0;36mfetch_news\u001b[0;34m(ticker)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdate_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'publishedAt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'publishedAt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wKCVksAC_65e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}